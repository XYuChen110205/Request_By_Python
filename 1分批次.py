import requests
import re
from lxml import etree
from datetime import datetime
import os
import json
import time


def extract_articles_from_12371(url):
    """
    ä¸“é—¨é’ˆå¯¹12371.cnç½‘ç«™çš„JavaScriptåŠ¨æ€å†…å®¹æå–
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'

        # ä½¿ç”¨æ­£åˆ™ç›´æ¥æå–
        pattern = r"'link_add':'([^']+)','title':'([^']+)'"
        matches = re.findall(pattern, response.text)

        articles = []
        for link, title in matches:
            articles.append({
                'title': title,
                'url': link
            })

        # å»é‡
        unique_articles = []
        seen_titles = set()
        for article in articles:
            if article['title'] not in seen_titles:
                seen_titles.add(article['title'])
                unique_articles.append(article)

        return unique_articles
    except Exception as e:
        print(f"æå–æ–‡ç« åˆ—è¡¨å¤±è´¥ {url}: {e}")
        return []


def get_article_content(article_url):
    """
    è·å–å•ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    try:
        response = requests.get(article_url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        tree = etree.HTML(response.text)

        # æå–æ–‡ç« æ ‡é¢˜
        title = tree.xpath('//h1/text() | //title/text()')
        title = title[0].strip() if title else "æ— æ ‡é¢˜"

        # æ›´ç²¾ç¡®çš„å†…å®¹æå–ï¼ˆæ’é™¤å¯¼èˆªã€é¡µè„šç­‰ï¼‰
        content_elements = tree.xpath('''
            //div[contains(@class, "content")]//text() |
            //div[contains(@class, "article")]//text() |
            //div[contains(@class, "text")]//text() |
            //p[not(ancestor::div[contains(@class, "nav")])]//text()
        ''')

        # è¿‡æ»¤æ‰æ— å…³å†…å®¹
        filtered_content = []
        for text in content_elements:
            text = text.strip()
            if (text and
                    len(text) > 5 and  # è¿‡æ»¤è¿‡çŸ­çš„æ–‡æœ¬
                    'æ¬¢è¿ä½¿ç”¨æ‰‹æœºæµè§ˆ' not in text and
                    'çƒ­æœ' not in text and
                    'å…³äºæˆ‘ä»¬' not in text and
                    'è”ç³»æˆ‘ä»¬' not in text and
                    'ç½‘ç«™åœ°å›¾' not in text and
                    'ç”¨æˆ·è°ƒæŸ¥' not in text and
                    'å…±äº§å…šå‘˜ç½‘' not in text and
                    'äº¬ICPå¤‡' not in text):
                filtered_content.append(text)

        content = '\n'.join(filtered_content)

        if not content.strip():
            content = "æœªèƒ½è·å–åˆ°æ–‡ç« å†…å®¹"

        return {
            'title': title,
            'content': content,
            'url': article_url
        }
    except Exception as e:
        return {
            'title': 'è·å–å¤±è´¥',
            'content': f'è·å–æ–‡ç« å†…å®¹æ—¶å‡ºé”™: {str(e)}',
            'url': article_url
        }


def save_chunked_files(data, base_filename, chunk_size=100, file_type="json"):
    """
    å°†æ•°æ®åˆ†å—ä¿å­˜
    """
    os.makedirs(os.path.dirname(base_filename), exist_ok=True)

    for i in range(0, len(data), chunk_size):
        chunk = data[i:i + chunk_size]
        chunk_num = i // chunk_size + 1
        start_num = i + 1
        end_num = i + len(chunk)

        if file_type == "json":
            filename = f"{base_filename}_{start_num}-{end_num}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(chunk, f, ensure_ascii=False, indent=2)
        else:  # txtæ ¼å¼
            filename = f"{base_filename}_{start_num}-{end_num}.txt"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"=== {os.path.basename(base_filename)} {start_num}-{end_num} ===\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 60 + "\n\n")

                for j, item in enumerate(chunk, 1):
                    f.write(f"ã€ç¬¬{j}æ¡ã€‘{item['title']}\n")
                    f.write(f"é“¾æ¥: {item['url']}\n")
                    if 'content' in item:
                        f.write("-" * 40 + "\n")
                        f.write(f"{item['content']}\n")
                    f.write("\n" + "=" * 50 + "\n\n")

        print(f"å·²ä¿å­˜: {filename} (å…±{len(chunk)}æ¡)")


def crawl_and_save_data():
    """
    ä¸»çˆ¬å–å’Œä¿å­˜å‡½æ•°
    """
    print("å¼€å§‹çˆ¬å–å…šåŠ¡æ–‡ç« ...")

    categories = {
        'å…šç« ': 'https://www.12371.cn/special/dnfg/',
        'æ¡ä¾‹': 'https://www.12371.cn/special/dnfg/tl/',
        'å‡†åˆ™': 'https://www.12371.cn/special/dnfg/zz/',
        'è§„å®š': 'https://www.12371.cn/special/dnfg/gd/',
        'åŠæ³•': 'https://www.12371.cn/special/dnfg/bf/',
        'è§„åˆ™': 'https://www.12371.cn/special/dnfg/gz/',
        'ç»†åˆ™': 'https://www.12371.cn/special/dnfg/xz/',
        'è§„èŒƒæ€§æ–‡ä»¶': 'https://www.12371.cn/special/zcwj/'
    }

    # åˆ›å»ºä¸»æ–‡ä»¶å¤¹
    os.makedirs("å…šåŠ¡é“¾æ¥æ–‡ä»¶å¤¹", exist_ok=True)
    os.makedirs("å…šåŠ¡æ–‡ç« å†…å®¹æ–‡ä»¶å¤¹", exist_ok=True)

    # æ”¶é›†æ‰€æœ‰æ–‡ç« æ•°æ®
    all_articles_data = {}

    print("æ­£åœ¨è·å–å„åˆ†ç±»æ–‡ç« é“¾æ¥...")
    for name, url in categories.items():
        print(f"æ­£åœ¨å¤„ç† {name} åˆ†ç±»...")
        articles = extract_articles_from_12371(url)
        all_articles_data[name] = articles
        print(f"  {name}: æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

        # åˆç†çš„å»¶è¿Ÿ
        time.sleep(1)

    print("\nå¼€å§‹ä¿å­˜é“¾æ¥æ–‡ä»¶...")

    # ä¿å­˜é“¾æ¥æ–‡ä»¶
    for category, articles in all_articles_data.items():
        if category == "è§„èŒƒæ€§æ–‡ä»¶":
            # è§„èŒƒæ€§æ–‡ä»¶åˆ†å—ä¿å­˜
            save_chunked_files(
                articles,
                f"å…šåŠ¡é“¾æ¥æ–‡ä»¶å¤¹/è§„èŒƒæ€§æ–‡ä»¶é“¾æ¥",
                chunk_size=100,
                file_type="json"
            )
        else:
            # å…¶ä»–åˆ†ç±»å•ä¸ªæ–‡ä»¶ä¿å­˜
            filename = f"å…šåŠ¡é“¾æ¥æ–‡ä»¶å¤¹/{category}é“¾æ¥æ–‡ä»¶.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            print(f"å·²ä¿å­˜: {filename}")

    print("\nå¼€å§‹çˆ¬å–å’Œä¿å­˜æ–‡ç« å†…å®¹...")

    # çˆ¬å–å’Œä¿å­˜æ–‡ç« å†…å®¹
    for category, articles in all_articles_data.items():
        print(f"\næ­£åœ¨å¤„ç† {category} å†…å®¹...")

        contents = []
        total_articles = len(articles)

        for i, article in enumerate(articles, 1):
            print(f"  çˆ¬å–è¿›åº¦: {i}/{total_articles} - {article['title'][:30]}...")

            content_data = get_article_content(article['url'])
            contents.append(content_data)

            # åˆç†çš„çˆ¬å–é€Ÿåº¦ï¼šæ¯2ç¯‡æ–‡ç« æš‚åœ1ç§’
            if i % 2 == 0:
                time.sleep(1)

            # æ¯10ç¯‡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if i % 10 == 0:
                print(f"   å·²å®Œæˆ {i}/{total_articles}")

        # ä¿å­˜å†…å®¹æ–‡ä»¶
        if category == "è§„èŒƒæ€§æ–‡ä»¶":
            # è§„èŒƒæ€§æ–‡ä»¶åˆ†å—ä¿å­˜
            save_chunked_files(
                contents,
                f"å…šåŠ¡æ–‡ç« å†…å®¹æ–‡ä»¶å¤¹/è§„èŒƒæ€§æ–‡ä»¶æ­£æ–‡",
                chunk_size=100,
                file_type="txt"
            )
        else:
            # å…¶ä»–åˆ†ç±»å•ä¸ªæ–‡ä»¶ä¿å­˜
            filename = f"å…šåŠ¡æ–‡ç« å†…å®¹æ–‡ä»¶å¤¹/{category}å†…å®¹æ–‡ä»¶.txt"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"=== {category}å†…å®¹æ±‡æ€» ===\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 60 + "\n\n")

                for i, content in enumerate(contents, 1):
                    f.write(f"ã€ç¬¬{i}ç¯‡ã€‘{content['title']}\n")
                    f.write(f"é“¾æ¥: {content['url']}\n")
                    f.write("-" * 40 + "\n")
                    f.write(f"{content['content']}\n")
                    f.write("\n" + "=" * 50 + "\n\n")

            print(f"å·²ä¿å­˜: {filename}")

    # ç”ŸæˆæŠ¥å‘Š
    generate_final_report(all_articles_data)


def generate_final_report(articles_data):
    """
    ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    """
    report_filename = "å…šåŠ¡æ–‡ç« çˆ¬å–æŠ¥å‘Š.txt"

    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write("=== å…šåŠ¡æ–‡ç« çˆ¬å–å®ŒæˆæŠ¥å‘Š ===\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")

        total_articles = 0
        total_content_success = 0

        f.write("ã€åˆ†ç±»ç»Ÿè®¡ã€‘\n")
        f.write("-" * 50 + "\n")
        f.write(f"{'åˆ†ç±»åç§°':<10} {'æ–‡ç« æ•°é‡':<8} {'æ–‡ä»¶ç±»å‹':<15}\n")
        f.write("-" * 50 + "\n")

        for category, articles in articles_data.items():
            article_count = len(articles)
            total_articles += article_count

            if category == "è§„èŒƒæ€§æ–‡ä»¶":
                file_type = "åˆ†å—æ–‡ä»¶"
                # è®¡ç®—åˆ†å—æ•°é‡
                chunk_count = (article_count + 99) // 100
                file_info = f"{chunk_count}ä¸ªåˆ†å—æ–‡ä»¶"
            else:
                file_type = "å•ä¸ªæ–‡ä»¶"
                file_info = "1ä¸ªæ–‡ä»¶"

            f.write(f"{category:<10} {article_count:<8} {file_type:<15} {file_info}\n")

        f.write(f"\nã€æ€»ä½“ç»Ÿè®¡ã€‘\n")
        f.write("-" * 50 + "\n")
        f.write(f"æ€»æ–‡ç« æ•°: {total_articles}\n")
        f.write(f"åˆ†ç±»æ•°é‡: {len(articles_data)}\n")

        f.write(f"\nã€æ–‡ä»¶ç»“æ„ã€‘\n")
        f.write("-" * 50 + "\n")
        f.write("å…šåŠ¡é“¾æ¥æ–‡ä»¶å¤¹/\n")
        for category in articles_data.keys():
            if category == "è§„èŒƒæ€§æ–‡ä»¶":
                f.write("  â”œâ”€â”€ è§„èŒƒæ€§æ–‡ä»¶é“¾æ¥_1-100.json\n")
                f.write("  â”œâ”€â”€ è§„èŒƒæ€§æ–‡ä»¶é“¾æ¥_101-200.json\n")
                f.write("  â””â”€â”€ ... (ä»¥æ­¤ç±»æ¨)\n")
            else:
                f.write(f"  â”œâ”€â”€ {category}é“¾æ¥æ–‡ä»¶.json\n")

        f.write("\nå…šåŠ¡æ–‡ç« å†…å®¹æ–‡ä»¶å¤¹/\n")
        for category in articles_data.keys():
            if category == "è§„èŒƒæ€§æ–‡ä»¶":
                f.write("  â”œâ”€â”€ è§„èŒƒæ€§æ–‡ä»¶æ­£æ–‡_1-100.txt\n")
                f.write("  â”œâ”€â”€ è§„èŒƒæ€§æ–‡ä»¶æ­£æ–‡_101-200.txt\n")
                f.write("  â””â”€â”€ ... (ä»¥æ­¤ç±»æ¨)\n")
            else:
                f.write(f"  â”œâ”€â”€ {category}å†…å®¹æ–‡ä»¶.txt\n")

        f.write(f"\nã€è¯´æ˜ã€‘\n")
        f.write("-" * 50 + "\n")
        f.write("1. è§„èŒƒæ€§æ–‡ä»¶ç”±äºæ•°é‡è¾ƒå¤šï¼Œå·²æŒ‰æ¯100æ¡è¿›è¡Œåˆ†å—ä¿å­˜\n")
        f.write("2. å…¶ä»–åˆ†ç±»æ–‡ä»¶å•ç‹¬ä¿å­˜ä¸ºä¸€ä¸ªæ–‡ä»¶\n")
        f.write("3. çˆ¬å–é€Ÿåº¦å·²ä¼˜åŒ–ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›\n")
        f.write("4. æ‰€æœ‰æ–‡ä»¶å‡é‡‡ç”¨UTF-8ç¼–ç ä¿å­˜\n")

    print(f"\nå·²ç”ŸæˆæŠ¥å‘Š: {report_filename}")


def main():
    """
    ä¸»ç¨‹åº
    """
    print("å¼€å§‹æ‰§è¡Œå…šåŠ¡æ–‡ç« çˆ¬å–ä»»åŠ¡...")
    print("æ³¨æ„ï¼šçˆ¬å–è¿‡ç¨‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…")
    print("çˆ¬å–é€Ÿåº¦å·²ä¼˜åŒ–ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›\n")

    try:
        crawl_and_save_data()

        print("\n" + "=" * 60)
        print("âœ… çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
        print("=" * 60)
        print("\nç”Ÿæˆçš„æ–‡ä»¶ç»“æ„ï¼š")
        print("å…šåŠ¡é“¾æ¥æ–‡ä»¶å¤¹/")
        print("  â”œâ”€â”€ å…šç« é“¾æ¥æ–‡ä»¶.json")
        print("  â”œâ”€â”€ å‡†åˆ™é“¾æ¥æ–‡ä»¶.json")
        print("  â”œâ”€â”€ æ¡ä¾‹é“¾æ¥æ–‡ä»¶.json")
        print("  â”œâ”€â”€ è§„å®šé“¾æ¥æ–‡ä»¶.json")
        print("  â”œâ”€â”€ åŠæ³•é“¾æ¥æ–‡ä»¶.json")
        print("  â”œâ”€â”€ è§„åˆ™é“¾æ¥æ–‡ä»¶.json")
        print("  â”œâ”€â”€ ç»†åˆ™é“¾æ¥æ–‡ä»¶.json")
        print("  â””â”€â”€ è§„èŒƒæ€§æ–‡ä»¶é“¾æ¥_1-100.json (ç­‰åˆ†å—æ–‡ä»¶)")
        print("å…šåŠ¡æ–‡ç« å†…å®¹æ–‡ä»¶å¤¹/")
        print("  â”œâ”€â”€ å…šç« å†…å®¹æ–‡ä»¶.txt")
        print("  â”œâ”€â”€ å‡†åˆ™å†…å®¹æ–‡ä»¶.txt")
        print("  â”œâ”€â”€ æ¡ä¾‹å†…å®¹æ–‡ä»¶.txt")
        print("  â”œâ”€â”€ è§„å®šå†…å®¹æ–‡ä»¶.txt")
        print("  â”œâ”€â”€ åŠæ³•å†…å®¹æ–‡ä»¶.txt")
        print("  â”œâ”€â”€ è§„åˆ™å†…å®¹æ–‡ä»¶.txt")
        print("  â”œâ”€â”€ ç»†åˆ™å†…å®¹æ–‡ä»¶.txt")
        print("  â””â”€â”€ è§„èŒƒæ€§æ–‡ä»¶æ­£æ–‡_1-100.txt (ç­‰åˆ†å—æ–‡ä»¶)")
        print("\nå…šåŠ¡æ–‡ç« çˆ¬å–æŠ¥å‘Š.txt")
        # print("\nå¯ä»¥å®‰å¿ƒç¡è§‰äº†ï¼ğŸ˜Š")

    except Exception as e:
        print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•")


if __name__ == "__main__":
    main()